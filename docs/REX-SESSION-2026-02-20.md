# REX Session 2026-02-20 â€” Workstation Pi : DÃ©ploiement complet + VPN

## Contexte

Suite du dÃ©ploiement du Raspberry Pi 5 (16GB, SSD 256Go) comme **Mission Control / Workstation**.
Continuation de la session 2026-02-18 (voir `REX-SESSION-2026-02-18.md`).

---

## Architecture actuelle (Ã©tat fin de session)

### Serveurs

| Serveur | IP | RÃ´le | Ã‰tat |
|---|---|---|---|
| **Sese-AI** (OVH VPS) | 137.74.114.167 | Cerveau IA | âœ… OpÃ©rationnel |
| **Seko-VPN** (Ionos) | 87.106.30.160 | Headscale hub VPN | âš ï¸ Down (dÃ©ploiement ratÃ© autre session) |
| **Workstation Pi** (RPi5) | 192.168.1.8 (LAN) | Mission Control | âœ… Partiellement opÃ©rationnel |

### Services sur Sese-AI (VPS OVH â€” 137.74.114.167, port SSH 804)

Tous les containers Docker **healthy** :

| Container | Image | Ã‰tat |
|---|---|---|
| javisi_openclaw | ghcr.io/openclaw/openclaw:2026.2.15 | âœ… Up 2j |
| javisi_litellm | ghcr.io/berriai/litellm:v1.81.3-stable | âœ… Up 34h |
| javisi_n8n | docker.n8n.io/n8nio/n8n:2.7.3 | âœ… Up 36h |
| javisi_caddy | caddy:2.10.2-alpine | âœ… Up 34h |
| javisi_postgresql | postgres:18.1-bookworm | âœ… Up 2j |
| javisi_redis | redis:8.0-bookworm | âœ… Up 2j |
| javisi_qdrant | qdrant/qdrant:v1.16.3 | âœ… Up 2j |
| javisi_grafana | grafana/grafana:12.3.2 | âœ… Up 2j |
| javisi_victoriametrics | victoriametrics/victoria-metrics:v1.135.0 | âœ… Up 2j |
| javisi_loki | grafana/loki:3.6.5 | âœ… Up 2j |
| javisi_alloy | grafana/alloy:v1.13.0 | âœ… Up 2j |
| javisi_cadvisor | ghcr.io/google/cadvisor:0.55.1 | âœ… Up 2j |
| javisi_diun | crazymax/diun:4.31.0 | âœ… Up 2j |

### Services sur Workstation Pi (192.168.1.8, port SSH 22)

| Service | Version | Port | Ã‰tat |
|---|---|---|---|
| Mission Control | v1.1.0 | 4000 | âœ… active (running) |
| OpenCode | 1.2.8 | 3456 | âœ… active (running) |
| Caddy (xcaddy+OVH) | v2.10.2 | 80/443 | âŒ caddy.service not found |
| Claude Code CLI | 2.1.49 | â€” | âœ… installÃ©, OAuth Max Plan âœ… |
| Tailscale | installÃ© | â€” | âŒ Logged out (Headscale down) |

---

## Ce qui a Ã©tÃ© fait cette session

### 1. Corrections pre-dÃ©ploiement

- `workstation_pi_user` corrigÃ© : `pi` â†’ `mobuone`
- `vault_workstation_pi_ip: "192.168.1.8"` ajoutÃ© dans `secrets.yml`
- SSH key : `~/.ssh/seko-vpn-deploy` (sur Windows : `/c/Users/mmomb/.ssh/seko-vpn-deploy`)
- `ansible_become_pass` via `vault_workstation_become_pass: "Elikya2015"` dans vault + `hosts.yml`

### 2. DÃ©ploiement complet Pi (commit `249afff`)

**workstation-common** : âœ…
- Ubuntu 24.04 ARM64, hostname `workstation-pi`
- Node.js v22.22.0 (NodeSource)
- Docker CE, UFW (LAN 192.168.0.0/16 + Tailscale 100.64.0.0/10 autorisÃ©s)
- Arborescence `/opt/workstation/{configs,data,logs}`

**mission-control** : âœ… (avec fixes)
- Repo : `crshdn/mission-control` pinnÃ© `v1.1.0`
- Fix : `npm ci` (pas `--omit=dev` â€” tailwindcss est devDep requis pour build)
- Fix : ExecStart = `next start -p 4000` (pas `node .next/standalone/server.js` â€” repo sans `output:'standalone'`)
- Fix : healthcheck sur `/` (pas `/api/health` â€” 404)
- Artefact de build : `.next/BUILD_ID`

**opencode** : âœ… (avec fixes)
- Version `1.2.8` â€” config format changÃ©, `providers`/`workspace` supprimÃ©s au niveau root
- npm prefix : `/usr` (NodeSource installe dans `/usr/bin`, pas `/usr/local/bin`)
- Config minimale valide : `{"username": "mobuone"}`

**workstation-caddy** : âœ… (xcaddy ARM64)
- Ubuntu 24.04 n'a que Go 1.22 â†’ plugin `caddy-dns/ovh` requiert Go >= 1.24
- Fix : installation Go 1.24.2 ARM64 depuis `dl.google.com/go/`
- Build : `xcaddy v0.4.5` + `caddy v2.10.2` + `--with github.com/caddy-dns/ovh`
- Caddyfile : proxy `mc.ewutelo.cloud` â†’ :4000 et `oc.ewutelo.cloud` â†’ :3456

### 3. Claude Code CLI OAuth Max Plan (commit `45cb125`)

- Claude Code CLI v2.1.49 installÃ© via npm global dans `workstation-common`
- Auth OAuth faite manuellement via `claude` en SSH (PowerShell â†’ lien URL copiÃ© dans navigateur)
- Tokens sauvegardÃ©s dans `~/.claude/` sur le Pi â€” **persistants, auto-renouvelÃ©s**
- **Claude Code utilise le quota Max Plan, pas l'API billing**

### 4. OpenCode â†’ LiteLLM (commit `45cb125`)

- `ANTHROPIC_API_KEY` retirÃ© du service systemd
- `LITELLM_API_KEY` injectÃ© Ã  la place
- `opencode.json.j2` configurÃ© avec provider custom LiteLLM (OpenAI-compatible) :
  - Base URL : `https://llm.ewutelo.cloud/v1`
  - ModÃ¨les : `litellm/claude-sonnet` (dÃ©faut), `litellm/claude-haiku`
- **OpenCode passe par LiteLLM â†’ budget $5/jour centralisÃ©**

### 5. Headscale-node pour Ubuntu (commit `45cb125`, dÃ©ploiement incomplet)

- RÃ´le `headscale-node` corrigÃ© pour Ubuntu (Ã©tait hardcodÃ© Debian) :
  - `DISTRO=$(ansible_facts['distribution'] | lower)` dans l'URL GPG et le repo apt
- `headscale_hostname` : utilise `workstation_pi_hostname` au lieu de `prod_hostname`
- AjoutÃ© dans `playbooks/workstation.yml` (phase 1, avant mission-control)
- **Tailscale installÃ© sur le Pi mais non connectÃ©** â€” Seko-VPN (Headscale) est down

---

## Ce qui reste Ã  faire

### ðŸ”´ PrioritÃ© 1 â€” Remettre Seko-VPN en ligne

Le serveur Headscale (Ionos 87.106.30.160) est indisponible suite Ã  un dÃ©ploiement ratÃ© depuis une autre session Claude.

**Diagnostic Ã  faire :**
- AccÃ¨s console Ionos (KVM/VNC) si SSH impossible
- Identifier ce qui a cassÃ© (UFW lockout ? service crash ? mauvais deploy ?)
- RedÃ©marrer Headscale si nÃ©cessaire

**Une fois Seko-VPN remontÃ© :**
```bash
# GÃ©nÃ©rer une nouvelle clÃ© preauth (l'ancienne est expirÃ©e/utilisÃ©e)
# Sur Seko-VPN :
headscale preauthkeys create --user default --expiration 24h
# Mettre Ã  jour dans vault : headscale_auth_key: "nouvelle_cle"
ansible-vault edit inventory/group_vars/all/secrets.yml

# RedÃ©ployer Tailscale sur le Pi :
make deploy-role ROLE=headscale-node ENV=workstation
# ou :
wsl.exe -d Ubuntu -e bash -c "cd /home/asus/seko/VPAI && source .venv/bin/activate && ansible-playbook playbooks/workstation.yml --vault-password-file .vault_pass --tags headscale-node"
```

### ðŸ”´ PrioritÃ© 2 â€” Caddy non dÃ©marrÃ© sur le Pi

`systemctl status caddy` â†’ `Unit caddy.service could not be found`

Le binaire Caddy est buildÃ© (`/usr/bin/caddy`) mais le service systemd n'est pas installÃ©.
Le rÃ´le `workstation-caddy` n'a probablement pas Ã©tÃ© rejouÃ© aprÃ¨s les fixes.

**Fix :**
```bash
wsl.exe -d Ubuntu -e bash -c "cd /home/asus/seko/VPAI && source .venv/bin/activate && ansible-playbook playbooks/workstation.yml --vault-password-file .vault_pass --tags workstation-caddy"
```

VÃ©rifier ensuite :
- `systemctl status caddy` â†’ active
- `curl -k https://mc.ewutelo.cloud` â†’ Mission Control accessible
- `curl -k https://oc.ewutelo.cloud` â†’ OpenCode accessible

### ðŸŸ¡ PrioritÃ© 3 â€” OpenClaw â†” Mission Control connectivity

Une fois VPN + Caddy fonctionnels, configurer la communication :

**Architecture cible :**
```
Mission Control (Pi) â†â†’ OpenClaw (VPS)
    mc.ewutelo.cloud          openclaw.ewutelo.cloud
         â†“ VPN                       â†“ VPN
   Headscale mesh â†â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â†’ Headscale mesh
```

Mission Control doit connaÃ®tre l'URL d'OpenClaw. VÃ©rifier dans la config Mission Control :
```bash
cat /opt/workstation/configs/opencode/.env 2>/dev/null
# ou
grep -r 'openclaw\|OPENCLAW\|OPENCODE' /opt/workstation/
```

Potentiellement ajouter dans le `.env` de Mission Control :
```
OPENCLAW_URL=https://openclaw.ewutelo.cloud
OPENCLAW_API_KEY=<vault_openclaw_api_key>
```

### ðŸŸ¡ PrioritÃ© 4 â€” DNS records mc/oc dans Headscale

Une fois VPN actif, ajouter les records DNS Split dans Headscale pour que mc/oc soient accessibles depuis le mesh VPN :
```yaml
# Dans config Headscale sur Seko-VPN :
dns:
  extra_records:
    - name: "mc.ewutelo.cloud"
      type: A
      value: "<workstation_pi_tailscale_ip>"
    - name: "oc.ewutelo.cloud"
      type: A
      value: "<workstation_pi_tailscale_ip>"
```

### ðŸŸ¢ Nice to have â€” VÃ©rifier OVH credentials dans Caddy Pi

Le Caddyfile utilise `{env.OVH_APPLICATION_KEY}` etc. VÃ©rifier que les credentials OVH sont bien injectÃ©s dans le service Caddy (via EnvironmentFile ou Environment= dans le .service).

---

## Fichiers modifiÃ©s cette session (vs main avant session)

| Fichier | Changement |
|---|---|
| `inventory/hosts.yml` | + `ansible_become_pass` pour workstation |
| `inventory/group_vars/all/main.yml` | + `workstation_pi_become_pass`, user `mobuone` |
| `inventory/group_vars/all/secrets.yml` | + `vault_workstation_pi_ip`, `vault_workstation_become_pass` |
| `roles/workstation-common/tasks/main.yml` | + installation Claude Code CLI |
| `roles/workstation-common/defaults/main.yml` | + `workstation_claude_code_version: "2.1.49"` |
| `roles/opencode/templates/opencode.service.j2` | `ANTHROPIC_API_KEY` â†’ `LITELLM_API_KEY` |
| `roles/opencode/templates/opencode.json.j2` | Config provider LiteLLM custom |
| `roles/headscale-node/tasks/main.yml` | Debian â†’ Ubuntu/gÃ©nÃ©rique |
| `roles/headscale-node/defaults/main.yml` | hostname : `prod_hostname` â†’ `workstation_pi_hostname` |
| `playbooks/workstation.yml` | + rÃ´le `headscale-node`, commentaires mis Ã  jour |

**Dernier commit pushÃ© : `45cb125`** sur `main`

---

## PIÃ¨ges et REX techniques

### REX-W1 â€” npm prefix NodeSource
`npm install -g` avec NodeSource v22 installe dans `/usr/bin`, pas `/usr/local/bin`.
â†’ `opencode_npm_prefix: "/usr"` dans defaults.

### REX-W2 â€” Mission Control next.config.mjs
`crshdn/mission-control` n'a pas `output: 'standalone'` â†’ pas de `.next/standalone/server.js`.
â†’ ExecStart : `next start -p {{ mc_port }}` (via node_modules/.bin/next).
â†’ Artefact : `.next/BUILD_ID` (pas `.next/standalone/server.js`).
â†’ `npm ci` obligatoire (pas `--omit=dev`) â€” tailwindcss est devDep requis pour build.

### REX-W3 â€” OpenCode v1.2.8 config
Config schema changÃ© : `providers` et `workspace` ne sont plus des clÃ©s root valides.
â†’ Config minimale : `{"username": "mobuone"}`.
â†’ Auth : via `ANTHROPIC_API_KEY` env var OU `opencode auth login` OAuth.

### REX-W4 â€” xcaddy ARM64 Go version
Ubuntu 24.04 ARM64 = Go 1.22 â†’ insuffisant pour `caddy-dns/ovh` (requiert Go >= 1.24).
â†’ Installer Go 1.24.2 depuis `dl.google.com/go/go1.24.2.linux-arm64.tar.gz`.
â†’ xcaddy v0.4.5 depuis `/usr/local/go/bin/go install`.

### REX-W5 â€” Claude Code OAuth Max Plan
LiteLLM NE PEUT PAS utiliser le Max Plan â€” c'est OAuth browser-based.
â†’ Seul `claude` CLI supporte OAuth Max Plan (quota abonnement, pas API billing).
â†’ Auth : SSH dans tmux â†’ lancer `claude` â†’ copier URL dans navigateur Windows â†’ done.
â†’ Tokens persistants dans `~/.claude/`, auto-renouvelÃ©s.

### REX-W6 â€” Headscale preauth key usage unique
La clÃ© `headscale_auth_key` dans le vault est Ã  usage unique ET expirante.
â†’ AprÃ¨s utilisation ou expiration, en gÃ©nÃ©rer une nouvelle via `headscale preauthkeys create`.
â†’ Ne jamais rÃ©utiliser une ancienne clÃ©.

### REX-W7 â€” tailscale up bloque si Headscale down
`tailscale up --login-server=...` attend indÃ©finiment si le serveur est inaccessible.
â†’ Ansible timeout ou kill manuel nÃ©cessaire.
â†’ Toujours vÃ©rifier Headscale accessible avant de lancer le rÃ´le.

### REX-W8 â€” SSH key path selon environnement
- WSL Ubuntu : `~/.ssh/seko-vpn-deploy` = `/home/asus/.ssh/seko-vpn-deploy`
- Git Bash / PowerShell Windows : `/c/Users/mmomb/.ssh/seko-vpn-deploy`
- Ansible (WSL) lit depuis WSL â†’ utiliser chemins WSL dans `ansible.cfg`

---

## Commandes utiles pour la reprise

```bash
# SSH Pi (depuis Git Bash/PowerShell Windows)
ssh -i /c/Users/mmomb/.ssh/seko-vpn-deploy mobuone@192.168.1.8

# SSH VPS prod
ssh -i /c/Users/mmomb/.ssh/seko-vpn-deploy -p 804 mobuone@137.74.114.167

# Deploy depuis WSL
wsl.exe -d Ubuntu -e bash -c "cd /home/asus/seko/VPAI && source .venv/bin/activate && ansible-playbook playbooks/workstation.yml --vault-password-file .vault_pass --tags <role>"

# VÃ©rifier Ã©tat Pi
ssh -i /c/Users/mmomb/.ssh/seko-vpn-deploy mobuone@192.168.1.8 'systemctl status opencode mission-control --no-pager'

# VÃ©rifier Ã©tat VPS
ssh -i /c/Users/mmomb/.ssh/seko-vpn-deploy -p 804 mobuone@137.74.114.167 'docker ps --format "table {{.Names}}\t{{.Status}}"'

# Statut Tailscale Pi
ssh -i /c/Users/mmomb/.ssh/seko-vpn-deploy mobuone@192.168.1.8 'echo Elikya2015 | sudo -S tailscale status'
```
