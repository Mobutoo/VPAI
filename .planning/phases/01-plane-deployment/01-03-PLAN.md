---
wave: 3
depends_on: ["01-01a", "01-01b", "01-02a", "01-02b"]
files_modified:
  - roles/monitoring/templates/grafana/dashboards/plane-dashboard.json.j2
  - roles/monitoring/templates/grafana/provisioning/dashboards.yml
  - roles/monitoring/templates/grafana/alerts/plane-alerts.yml.j2
  - roles/backup-config/templates/zerobyte-config.yml.j2
  - roles/backup-config/defaults/main.yml
  - playbooks/smoke-tests.yml
  - docs/TROUBLESHOOTING.md
autonomous: true
---

# Plan 01-03: Monitoring, Backup, and Smoke Tests

**Goal**: Integrate Plane into VPAI observability stack (Grafana dashboards, alerts), add database to Zerobyte backup routine, and create smoke tests to validate end-to-end functionality including agent authentication.

**Requirements**: INFRA-05, MONITOR-01, MONITOR-02, MONITOR-03, MONITOR-04

## Context

VPAI infrastructure includes comprehensive observability via Grafana stack (VictoriaMetrics for metrics, Loki for logs, Alloy for collection). All containerized services have:
- cAdvisor metrics scraping (CPU/RAM/network per container)
- Loki log aggregation via Docker stdout
- Grafana dashboards for service-specific monitoring
- Alertmanager rules for critical failures

Plane requires integration into this existing stack:
- Grafana dashboard showing API request rate, latency P95, queue depth, container resources
- Alert rules for API downtime, PostgreSQL connection exhaustion, Redis memory pressure, worker queue backlog
- PostgreSQL backup includes plane_production database in nightly Zerobyte dump
- Smoke tests validate: API health, admin login, workspace creation, project/issue CRUD, agent token auth, Redis key isolation, Loki log capture

**Critical constraints:**
- Healthcheck endpoint unknown: Research found `/api/health` documented but unverified - may be `/health`, `/api/`, or `/api/v1/health` (needs testing in smoke tests)
- Redis key monitoring: Must validate no collisions with n8n/LiteLLM keys (documented open question from Plan 01-02)
- VictoriaMetrics query: Plane doesn't expose native metrics - dashboard uses cAdvisor container metrics + calculated rates
- Agent token validation: Must assert tokens are non-empty (prerequisite: checkpoint from Plan 01-02b confirms tokens populated)

## Tasks

<task id="01-03-T1" name="grafana-plane-dashboard">
Create roles/monitoring/templates/grafana/dashboards/plane-dashboard.json.j2 with 4 rows:

Row 1: Overview
- Panel: Plane API Uptime (gauge) - query: up{container_name="{{ project_name }}_plane_api"} - threshold: red <1, green 1
- Panel: Active Containers (stat) - query: count(up{container_name=~"{{ project_name }}_plane.*"}) - expected: 3 (web, api, worker)
- Panel: Total Requests (graph, 5min rate) - query: rate(container_network_receive_bytes_total{container_name="{{ project_name }}_plane_web"}[5m]) - proxy for request volume

Row 2: Performance
- Panel: API Response Time P95 (graph) - calculated from cAdvisor CPU wait time as proxy (no native Plane metrics)
- Panel: Worker Queue Depth (graph) - Redis LLEN query on celery:* keys via VictoriaMetrics redis_exporter (if available) or stat N/A with note "Enable redis_exporter for queue metrics"
- Panel: Database Connections (graph) - PostgreSQL pg_stat_database query for plane_production numbackends

Row 3: Resources
- Panel: Container Memory Usage (graph, stacked) - query: container_memory_usage_bytes{container_name=~"{{ project_name }}_plane.*"} - show web, api, worker separately
- Panel: Container CPU Usage (graph) - query: rate(container_cpu_usage_seconds_total{container_name=~"{{ project_name }}_plane.*"}[5m])
- Panel: Disk I/O (graph) - container_fs_reads_bytes_total + writes for uploads volume

Row 4: Logs
- Panel: Error Logs (logs panel) - Loki query: {container_name=~".*plane.*"} |= "ERROR" - last 1h
- Panel: API Request Logs (logs panel) - Loki query: {container_name="{{ project_name }}_plane_api"} |= "POST\|GET\|PUT\|DELETE" - last 15min

Dashboard variables:
- $project_name from Grafana templating (default: {{ project_name }})

Dashboard metadata:
- Title: "Plane - Operational Intelligence"
- UID: plane-overview
- Tags: [plane, project-management, applications]
</task>

<task id="01-03-T2" name="grafana-alert-rules">
Create roles/monitoring/templates/grafana/alerts/plane-alerts.yml.j2 with Alertmanager rules:

Alert: PlaneAPIDown
- Condition: up{container_name="{{ project_name }}_plane_api"} == 0 for 2 minutes
- Severity: critical
- Action: Send Telegram notification via n8n webhook
- Message: "üö® Plane API is DOWN - operational intelligence unavailable"

Alert: PlaneWebDown
- Condition: up{container_name="{{ project_name }}_plane_web"} == 0 for 2 minutes
- Severity: warning
- Action: Telegram notification
- Message: "‚ö†Ô∏è Plane Web UI is DOWN - API still accessible"

Alert: PlanePostgreSQLConnectionsHigh
- Condition: sum(pg_stat_database_numbackends{datname="plane_production"}) > 80 for 5 minutes
- Severity: warning
- Action: Telegram notification
- Message: "‚ö†Ô∏è Plane PostgreSQL connections at {{ $value }}/100 - investigate slow queries"

Alert: PlaneRedisMemoryHigh
- Condition: redis_memory_used_bytes / redis_memory_max_bytes > 0.8 for 10 minutes
- Severity: warning
- Action: Telegram notification
- Message: "‚ö†Ô∏è Redis memory at {{ $value }}% - Plane cache/sessions may be evicted"
- Note: This alert covers ALL Redis users (n8n, LiteLLM, Plane) - not Plane-specific

Alert: PlaneWorkerQueueBacklog
- Condition: redis_list_length{key=~"celery:*"} > 100 for 15 minutes
- Severity: warning
- Action: Telegram notification
- Message: "‚ö†Ô∏è Plane worker queue has {{ $value }} pending tasks - check worker logs"
- Note: Requires redis_exporter enabled (defer to v2 if not available)

All alerts route to Telegram via n8n workflow webhook (existing VPAI pattern).
</task>

<task id="01-03-T3" name="backup-integration">
Add Plane database to roles/backup-config/templates/zerobyte-config.yml.j2:

In postgresql_databases array, add:
```yaml
  - name: plane_production
    owner: plane
    backup_schedule: "0 2 * * *"  # Daily 2 AM UTC
    retention_days: 7
```

Update roles/backup-config/defaults/main.yml:
- Add comment: "Plane database backup - includes workspace, projects, issues, attachments metadata (file blobs in Docker volume)"

Add explicit phase limitation comment in defaults/main.yml:
```yaml
# PHASE LIMITATION: Uploads volume backup deferred to Phase 2
# Zerobyte v0.16 file backup support unconfirmed. INFRA-05 (file backup requirement)
# intentionally not claimed in Phase 1 - PostgreSQL backup only (metadata).
# If Zerobyte supports file backups, add:
#   file_backups:
#     - source: "/opt/{{ project_name }}/data/plane/uploads"
#       destination: "backups/plane/uploads"
#       schedule: "0 3 * * *"
#       retention_days: 7
# Otherwise: v2 enhancement for dedicated Docker volume backup tooling.
```

Do NOT add file_backups section to zerobyte-config.yml.j2 in this plan - file backup is explicitly out of scope for Phase 1.

DOCS INTEGRATION: Add section to TROUBLESHOOTING.md (subsection 5 under Plane section created in T5):
- Title: "Backup restoration procedure"
- Commands: Restore plane_production database, verify file upload access
- Expected outcome: Plane data restored from nightly backup
</task>

<task id="01-03-T4" name="smoke-tests-and-log-validation">
Create or extend playbooks/smoke-tests.yml with Plane validation tasks (MERGED from T4+T6):

Add new play:
```yaml
- name: Plane Smoke Tests
  hosts: prod
  gather_facts: false
  tasks:
    - name: Test Plane API health endpoint
      ansible.builtin.uri:
        url: "https://work.{{ domain_name }}/api/health"
        method: GET
        status_code: 200
        validate_certs: true
        timeout: 10
      register: health_check
      failed_when: health_check.status != 200
      tags: [smoke, plane]

    - name: Fallback - Test root endpoint if /api/health fails
      ansible.builtin.uri:
        url: "https://work.{{ domain_name }}/"
        method: GET
        status_code: 200
      when: health_check is failed
      register: root_check
      tags: [smoke, plane]

    - name: Verify Plane UI is accessible from VPN
      ansible.builtin.assert:
        that:
          - health_check.status == 200 or root_check.status == 200
        fail_msg: "Plane UI not accessible - neither /api/health nor / returned 200"
        success_msg: "‚úì Plane UI accessible at work.{{ domain_name }}"
      tags: [smoke, plane]

    - name: Test workspace API with admin token
      ansible.builtin.uri:
        url: "https://work.{{ domain_name }}/api/v1/workspaces/"
        method: GET
        headers:
          X-API-Key: "{{ vault_plane_admin_api_token }}"
        status_code: 200
      when: vault_plane_admin_api_token != "REPLACE_AFTER_FIRST_LOGIN"
      register: workspace_check
      tags: [smoke, plane]

    - name: Verify workspace javisi exists
      ansible.builtin.assert:
        that:
          - workspace_check.json | selectattr('slug', 'equalto', 'javisi') | list | length > 0
        fail_msg: "Workspace 'javisi' not found - run plane-provision playbook"
        success_msg: "‚úì Workspace 'javisi' exists"
      when: workspace_check is not skipped
      tags: [smoke, plane]

    - name: Test agent API token authentication
      ansible.builtin.uri:
        url: "https://work.{{ domain_name }}/api/v1/workspaces/"
        method: GET
        headers:
          X-API-Key: "{{ vault_plane_agent_tokens.concierge }}"
        status_code: 200
      when: vault_plane_agent_tokens.concierge != ""
      register: agent_auth_check
      failed_when: agent_auth_check.status != 200
      tags: [smoke, plane]

    - name: Assert agent tokens are populated (AUTH-03 requirement)
      ansible.builtin.assert:
        that:
          - vault_plane_agent_tokens.concierge != ""
          - vault_plane_agent_tokens.imhotep != ""
          - vault_plane_agent_tokens.thot != ""
        fail_msg: "Agent tokens are empty - provision script not executed. See docs/GUIDE-PLANE-PROVISIONING.md"
        success_msg: "‚úì Agent tokens populated and validated"
      tags: [smoke, plane]

    - name: Check Redis for Plane key collisions
      ansible.builtin.command:
        cmd: docker exec {{ project_name }}_redis redis-cli KEYS "*"
      register: redis_keys
      changed_when: false
      tags: [smoke, plane]

    - name: Analyze Redis key patterns
      ansible.builtin.debug:
        msg: |
          Plane keys detected: {{ redis_keys.stdout_lines | select('match', '^(session|cache|celery):') | list | length }}
          n8n keys: {{ redis_keys.stdout_lines | select('match', '^n8n:') | list | length }}
          LiteLLM keys: {{ redis_keys.stdout_lines | select('match', '^litellm:') | list | length }}
          Total keys: {{ redis_keys.stdout_lines | length }}
          WARNING: Collision if Plane keys overlap with n8n/LiteLLM namespaces
      tags: [smoke, plane]

    - name: Check all 3 Plane containers are running
      ansible.builtin.command:
        cmd: docker ps --filter "name={{ project_name }}_plane" --filter "status=running" --format "{{ '{{' }}.Names{{ '}}' }}"
      register: plane_containers
      changed_when: false
      failed_when: plane_containers.stdout_lines | length != 3
      tags: [smoke, plane]

    - name: Verify all Plane container healthchecks are green
      ansible.builtin.command:
        cmd: docker inspect {{ project_name }}_plane_web {{ project_name }}_plane_api {{ project_name }}_plane_worker --format "{{ '{{' }}.Name{{ '}}' }}: {{ '{{' }}.State.Health.Status{{ '}}' }}"
      register: healthchecks
      changed_when: false
      failed_when: "'unhealthy' in healthchecks.stdout or 'starting' in healthchecks.stdout"
      tags: [smoke, plane]

    - name: Display Plane healthcheck status
      ansible.builtin.debug:
        msg: "{{ healthchecks.stdout_lines }}"
      tags: [smoke, plane]

    - name: Verify Plane logs in Loki (MERGED from T6)
      ansible.builtin.uri:
        url: "http://localhost:3100/loki/api/v1/query"
        method: GET
        body_format: json
        body:
          query: '{container_name=~".*plane.*"}'
          limit: 10
      register: loki_logs
      failed_when: loki_logs.json.data.result | length == 0
      tags: [smoke, plane, monitoring]

    - name: Display Plane log streams in Loki
      ansible.builtin.debug:
        msg: "‚úì Found {{ loki_logs.json.data.result | length }} Plane log streams in Loki"
      tags: [smoke, plane, monitoring]

    - name: Check Plane container resource usage
      ansible.builtin.command:
        cmd: docker stats --no-stream --format "{{ '{{' }}.Name{{ '}}' }}: CPU {{ '{{' }}.CPUPerc{{ '}}' }} | MEM {{ '{{' }}.MemUsage{{ '}}' }}" {{ project_name }}_plane_web {{ project_name }}_plane_api {{ project_name }}_plane_worker
      register: container_stats
      changed_when: false
      tags: [smoke, plane]

    - name: Display Plane resource usage
      ansible.builtin.debug:
        msg: "{{ container_stats.stdout_lines }}"
      tags: [smoke, plane]
```

All tasks use FQCN and have changed_when: false for checks.

CRITICAL (AUTH-03): Task "Assert agent tokens are populated" ensures tokens are not empty - this validates the checkpoint from Plan 01-02b was completed correctly.
</task>

<task id="01-03-T5" name="troubleshooting-documentation">
Add Plane section to docs/TROUBLESHOOTING.md:

Section number: 45 (or next available)
Title: "Plane Deployment & Operation"

Subsections:
1. **Healthcheck endpoint discovery**:
   - Issue: Plane API healthcheck fails, unclear which endpoint to use
   - Research: Documentation says /api/health but may be /health or /api/ or /api/v1/health
   - Solution: Test all variants in smoke tests, document working endpoint
   - Verification: `curl -I https://work.{{ domain_name }}/api/health` returns 200
   - Update: Smoke test fallback chain tests multiple endpoints

2. **Redis key collision with n8n/LiteLLM**:
   - Issue: Plane shares Redis with other services, no native key prefix support
   - Symptom: n8n session invalidation, Plane logout loops, Redis MONITOR showing overwrites
   - Diagnosis: `docker exec {{ project_name }}_redis redis-cli KEYS '*' | grep -E '^(session|cache):'`
   - Solution v1: Monitor during smoke tests (collision risk LOW - different key patterns)
   - Solution v2: If collisions detected, add dedicated Redis container for Plane (64MB overhead)
   - Prevention: Smoke test includes Redis key analysis

3. **SECRET_KEY missing causes JWT failures**:
   - Issue: Plane API returns 500 on login, logs show "SECRET_KEY not set"
   - Symptom: UI loads but authentication fails, JWT decode errors in plane-api logs
   - Diagnosis: `docker logs {{ project_name }}_plane_api 2>&1 | grep -i secret`
   - Solution: Verify plane.env has SECRET_KEY={{ vault_plane_secret_key }} and value is not empty
   - Verification: `docker exec {{ project_name }}_plane_api printenv SECRET_KEY` returns 50-char string
   - REX: v1.2.2 requires SECRET_KEY for Django cryptographic operations, no auto-generation

4. **CORS errors - WEB_URL mismatch**:
   - Issue: Plane UI loads but API calls fail with CORS preflight errors
   - Symptom: Browser console shows "Access-Control-Allow-Origin" errors, OPTIONS requests fail 403
   - Diagnosis: Check WEB_URL env var matches actual access URL (protocol + domain)
   - Solution: Ensure plane.env has WEB_URL=https://work.{{ domain_name }} and CORS_ALLOWED_ORIGINS=https://work.{{ domain_name }} (exact match)
   - Verification: `curl -I -H "Origin: https://work.{{ domain_name }}" https://work.{{ domain_name }}/api/v1/`
   - REX: HTTP vs HTTPS mismatch common if Caddy TLS not configured

5. **Backup restoration procedure** (from T3 integration):
   - Issue: Need to restore Plane data from backup after data loss
   - Commands: `zerobyte restore plane_production --date YYYY-MM-DD`, verify file upload directory
   - Expected outcome: Database restored, file uploads accessible
   - Verification: Login to Plane UI, check projects/issues exist, test file upload

6. **plane-worker crash loop - Celery broker missing**:
   - Issue: plane-worker container restarts continuously, logs show "No broker URL"
   - Symptom: Background tasks don't process (email notifications, webhooks), worker healthcheck fails
   - Diagnosis: `docker logs {{ project_name }}_plane_worker 2>&1 | grep -i celery`
   - Solution: Verify plane.env has CELERY_BROKER_URL=redis://redis:6379/1
   - Verification: `docker exec {{ project_name }}_plane_worker celery -A plane inspect ping` returns pong
   - REX: Redis database 1 for Celery broker, database 0 for cache (separation prevents key collision)

7. **File upload fails without object storage**:
   - Issue: Uploading attachments returns 500 error or fails silently
   - Symptom: Upload button works but files never appear in issue
   - Diagnosis: Check plane.env has USE_MINIO=0 and volume mount exists: `docker inspect {{ project_name }}_plane_web | grep -i /plane/media`
   - Solution: Verify /opt/{{ project_name }}/data/plane/uploads directory exists with ownership 1000:1000
   - Verification: Test upload in Plane UI, check `ls -la /opt/{{ project_name }}/data/plane/uploads/`
   - REX: v1 uses local filesystem storage (simpler than MinIO), volume must be writable by container UID 1000

8. **VPN ACL - 403 errors from VPN clients**:
   - Issue: Plane UI inaccessible from VPN with 403 Forbidden, but curl from VPS works
   - Symptom: Browser shows Caddy VPN error page even though Tailscale connected
   - Diagnosis: Check Caddyfile @blocked_plane matcher includes BOTH {{ caddy_vpn_cidr }} and {{ caddy_docker_frontend_cidr }}
   - Solution: HTTP/3 QUIC traffic has source IP replaced by Docker gateway (172.20.1.1) - matcher needs both CIDRs
   - Verification: `curl -v https://work.{{ domain_name }}` from VPN client, check response headers for x-forwarded-for
   - REX: Documented in docs/GUIDE-CADDY-VPN-ONLY.md - affects ALL VPN-only services using HTTP/3

9. **Provisioning fails - admin token invalid**:
   - Issue: plane-provision playbook fails with 401 Unauthorized on API calls
   - Symptom: Workspace creation task fails, logs show "Invalid authentication credentials"
   - Diagnosis: Check vault_plane_admin_api_token value in secrets.yml
   - Solution: If value is "REPLACE_AFTER_FIRST_LOGIN", follow docs/GUIDE-PLANE-PROVISIONING.md manual steps
   - Verification: `curl -H "X-API-Key: <token>" https://work.{{ domain_name }}/api/v1/workspaces/` returns 200
   - REX: Plane API token creation is manual-only in v1 (no programmatic endpoint found in v1.2.2 API docs)

Format: Follow existing TROUBLESHOOTING.md style with Issue/Symptom/Diagnosis/Solution/Verification/REX structure.
</task>

## Verification Criteria

After execution:

1. **Grafana dashboard created**:
   - `plane-dashboard.json.j2` exists with 4 rows and 12+ panels
   - Dashboard uses VictoriaMetrics queries for container metrics
   - Variables use {{ project_name }} for portability
   - Dashboard provisioning config updated in dashboards.yml

2. **Alert rules configured**:
   - `plane-alerts.yml.j2` has 5 alert definitions (API down, web down, DB connections, Redis memory, worker queue)
   - All alerts route to Telegram webhook
   - Conditions use appropriate thresholds (2min for critical, 5-15min for warnings)

3. **Backup integration**:
   - zerobyte-config.yml includes plane_production database in postgresql_databases array
   - backup-config/defaults/main.yml has phase limitation comment documenting that INFRA-05 (file backup) is out of scope for Phase 1 (BLOCKER #1 fix)
   - Uploads volume backup explicitly deferred to Phase 2 or v2 enhancement
   - Retention set to 7 days matching other VPAI services
   - TROUBLESHOOTING.md has backup restoration subsection

4. **Smoke tests comprehensive (T4+T6 merged)**:
   - smoke-tests.yml has Plane play with 11+ tasks
   - Tests cover: health endpoint (with fallback), UI accessibility, workspace API auth, agent token auth, agent token population assertion, Redis key analysis, container healthchecks, Loki log validation, resource usage
   - All tasks use FQCN and have proper error handling
   - Redis collision detection included
   - Agent token assertion validates AUTH-03 requirement
   - Loki validation confirms log aggregation working

5. **Troubleshooting documented**:
   - TROUBLESHOOTING.md section 45 (Plane) has 9 subsections
   - Each subsection follows Issue/Symptom/Diagnosis/Solution/Verification/REX pattern
   - Covers all known pitfalls from research plus backup restoration
   - User-observable language (UI accessible, containers green, auth working)

6. **Code quality**:
   - All queries use Jinja2 variables ({{ project_name }}, {{ domain_name }})
   - Grafana dashboard is valid JSON with templating
   - Alert rules use Prometheus query syntax
   - Smoke tests handle failure cases (fallback endpoints, skipped tasks when provisioning incomplete)
   - Success messages use ‚úì prefix for clear pass/fail visibility

## Must-Haves

Derived from phase goal: "Plane UI returns HTTP 200 from VPN, all 3 container healthchecks green, agent auth validated, logs flowing to Loki, database in daily backup"

1. **UI accessible from VPN**: Smoke test "Verify Plane UI is accessible from VPN" passes with ‚úì message
2. **Public access blocked**: VPN-only enforcement prevents unauthorized access (tested via Caddy config)
3. **All containers healthy**: Smoke test "Verify all Plane container healthchecks are green" passes (3/3 green)
4. **Agent authentication working**: Smoke test validates agent token (concierge) authenticates successfully to API
5. **Agent tokens populated**: Smoke test asserts vault_plane_agent_tokens values are non-empty (AUTH-03 requirement)
6. **Logs centralized**: Smoke test confirms Plane application logs flow to Loki (observability complete)
7. **Database backup operational**: PostgreSQL plane_production database included in nightly Zerobyte backup routine (uploads volume backup explicitly deferred - see BLOCKER #1 fix)
8. **Critical alerts configured**: API downtime and resource exhaustion trigger Telegram notifications within 2 minutes
9. **Troubleshooting knowledge captured**: All known pitfalls documented with diagnosis/solution steps in user-observable language
