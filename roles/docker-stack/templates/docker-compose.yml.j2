# {{ ansible_managed }}
# docker-compose.yml â€” Phase B: Applications, Monitoring, System
# Infrastructure services (PG, Redis, Qdrant, Caddy) are managed by docker-compose-infra.yml
# Networks created by docker-compose-infra.yml (external: true)

services:
  # === APPLICATION LAYER ===
  n8n:
    image: {{ n8n_image }}
    container_name: {{ project_name }}_n8n
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    env_file:
      - /opt/{{ project_name }}/configs/n8n/n8n.env
    volumes:
      - /opt/{{ project_name }}/data/n8n:/home/node/.n8n
    networks:
      - backend
      - egress
    deploy:
      resources:
        limits:
          memory: {{ n8n_memory_limit }}
          cpus: "{{ n8n_cpu_limit }}"
        reservations:
          memory: {{ n8n_memory_reservation }}
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:5678/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

  litellm:
    image: {{ litellm_image }}
    container_name: {{ project_name }}_litellm
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    env_file:
      - /opt/{{ project_name }}/configs/litellm/litellm.env
    volumes:
      - /opt/{{ project_name }}/configs/litellm/litellm_config.yaml:/app/config.yaml:ro
    command: --config /app/config.yaml --port 4000
    networks:
      - backend
      - egress
    deploy:
      resources:
        limits:
          memory: {{ litellm_memory_limit }}
          cpus: "{{ litellm_cpu_limit }}"
        reservations:
          memory: {{ litellm_memory_reservation }}
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import urllib.request; r=urllib.request.Request('http://127.0.0.1:4000/health',headers={'Authorization':'Bearer '+__import__('os').environ['LITELLM_MASTER_KEY']});urllib.request.urlopen(r)\" || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 90s

  openclaw:
    image: {{ openclaw_image }}
    container_name: {{ project_name }}_openclaw
    restart: unless-stopped
    init: true
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    env_file:
      - /opt/{{ project_name }}/configs/openclaw/openclaw.env
    volumes:
      - /opt/{{ project_name }}/data/openclaw:/home/node/.openclaw
      # Docker socket required for sandbox: Gateway spawns isolated containers
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - backend
      - egress
    deploy:
      resources:
        limits:
          memory: {{ openclaw_memory_limit }}
          cpus: "{{ openclaw_cpu_limit }}"
          pids: 512
        reservations:
          memory: {{ openclaw_memory_reservation }}
    # OpenClaw Gateway healthcheck: verify the WebSocket control plane is responsive
    healthcheck:
      test: ["CMD-SHELL", "node -e \"const h=require('http');h.get('http://127.0.0.1:{{ openclaw_gateway_port }}/',r=>{process.exit(r.statusCode<500?0:1)}).on('error',()=>process.exit(1))\""]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # === MONITORING LAYER ===
  cadvisor:
    image: {{ cadvisor_image }}
    container_name: {{ project_name }}_cadvisor
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - DAC_OVERRIDE
      - FOWNER
    command:
      - "--docker_only=true"
      - "--housekeeping_interval=30s"
      - "--disable_metrics=advtcp,cpu_topology,cpuset,hugetlb,memory_numa,process,referenced_memory,resctrl,sched,tcp,udp"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /run/containerd/containerd.sock:/run/containerd/containerd.sock:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          memory: {{ cadvisor_memory_limit }}
          cpus: "{{ cadvisor_cpu_limit }}"
        reservations:
          memory: {{ cadvisor_memory_reservation }}
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:8080/healthz || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  victoriametrics:
    image: {{ victoriametrics_image }}
    container_name: {{ project_name }}_victoriametrics
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
      - DAC_OVERRIDE
      - FOWNER
    command:
      - "-storageDataPath=/storage"
      - "-retentionPeriod=30d"
      - "-httpListenAddr=:8428"
    volumes:
      - /opt/{{ project_name }}/data/victoriametrics:/storage
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          memory: {{ victoriametrics_memory_limit }}
          cpus: "{{ victoriametrics_cpu_limit }}"
        reservations:
          memory: {{ victoriametrics_memory_reservation }}
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://127.0.0.1:8428/-/healthy || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  loki:
    image: {{ loki_image }}
    container_name: {{ project_name }}_loki
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
      - DAC_OVERRIDE
      - FOWNER
    volumes:
      - /opt/{{ project_name }}/data/loki:/loki
      - /opt/{{ project_name }}/configs/loki/loki-config.yaml:/etc/loki/local-config.yaml:ro
    command: -config.file=/etc/loki/local-config.yaml -target=all
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          memory: {{ loki_memory_limit }}
          cpus: "{{ loki_cpu_limit }}"
        reservations:
          memory: {{ loki_memory_reservation }}
    # NOTE: grafana/loki 3.6.5 is distroless (no wget/curl/test/ls)
    # Use built-in 'loki -health' command added in v3.6.5 (backport PR #20590)
    healthcheck:
      test: ["CMD", "/usr/bin/loki", "-health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 60s

  alloy:
    image: {{ alloy_image }}
    container_name: {{ project_name }}_alloy
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
      - SYS_PTRACE
      - DAC_READ_SEARCH
    volumes:
      - /opt/{{ project_name }}/configs/alloy/config.alloy:/etc/alloy/config.alloy:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /var/log:/var/log:ro
    command: run --server.http.listen-addr=0.0.0.0:12345 /etc/alloy/config.alloy
    networks:
      - backend
      - monitoring
    deploy:
      resources:
        limits:
          memory: {{ alloy_memory_limit }}
          cpus: "{{ alloy_cpu_limit }}"
        reservations:
          memory: {{ alloy_memory_reservation }}
    # NOTE: grafana/alloy may not have wget/curl
    # Process check as fallback; HTTP check done via smoke tests from host
    healthcheck:
      test: ["CMD-SHELL", "kill -0 1 || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s

  grafana:
    image: {{ grafana_image }}
    container_name: {{ project_name }}_grafana
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    environment:
      GF_SECURITY_ADMIN_USER: "{{ grafana_admin_user }}"
      GF_SECURITY_ADMIN_PASSWORD: "{{ grafana_admin_password }}"
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_SERVER_ROOT_URL: "https://{{ caddy_grafana_domain }}/"
    volumes:
      - /opt/{{ project_name }}/data/grafana:/var/lib/grafana
      - /opt/{{ project_name }}/configs/grafana/provisioning:/etc/grafana/provisioning:ro
      - /opt/{{ project_name }}/configs/grafana/dashboards:/var/lib/grafana/dashboards:ro
    networks:
      - frontend
      - monitoring
    deploy:
      resources:
        limits:
          memory: {{ grafana_memory_limit }}
          cpus: "{{ grafana_cpu_limit }}"
        reservations:
          memory: {{ grafana_memory_reservation }}
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:3000/api/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s

  # === SYSTEM ===
  diun:
    image: {{ diun_image }}
    container_name: {{ project_name }}_diun
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - DAC_OVERRIDE
      - FOWNER
    environment:
      TZ: "{{ timezone }}"
      CONFIG: "/diun.yml"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /opt/{{ project_name }}/data/diun:/data
      - /opt/{{ project_name }}/configs/diun/diun.yml:/diun.yml:ro
    deploy:
      resources:
        limits:
          memory: {{ diun_memory_limit }}
          cpus: "{{ diun_cpu_limit }}"
        reservations:
          memory: {{ diun_memory_reservation }}

networks:
  frontend:
    name: {{ project_name }}_frontend
    external: true
  backend:
    name: {{ project_name }}_backend
    external: true
  monitoring:
    name: {{ project_name }}_monitoring
    external: true
  egress:
    name: {{ project_name }}_egress
    external: true
