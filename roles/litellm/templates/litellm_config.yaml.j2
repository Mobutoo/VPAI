# {{ ansible_managed }}
# LiteLLM proxy configuration — Intelligent routing with tiered models
# Architecture: OpenClaw -> LiteLLM -> (Anthropic, OpenAI, OpenRouter, Google, BytePlus)
# Cache: Redis exact match + Qdrant semantic similarity

model_list:
  # ============================================================
  # TIER PREMIUM — Complex reasoning, architecture, code review
  # ============================================================
  - model_name: "claude-opus"
    litellm_params:
      model: "anthropic/claude-opus-4-20250514"
      api_key: "os.environ/ANTHROPIC_API_KEY"
      max_tokens: 16384
    model_info:
      max_input_tokens: 200000
      max_output_tokens: 16384
      metadata:
        tier: "premium"
        tags: ["code", "architecture", "reasoning", "review-premium"]

  - model_name: "claude-sonnet"
    litellm_params:
      model: "anthropic/claude-sonnet-4-20250514"
      api_key: "os.environ/ANTHROPIC_API_KEY"
      max_tokens: 8192
    model_info:
      max_input_tokens: 200000
      max_output_tokens: 8192
      metadata:
        tier: "medium"
        tags: ["writing", "analysis", "default", "code"]

  - model_name: "claude-haiku"
    litellm_params:
      model: "anthropic/claude-haiku-4-5-20251001"
      api_key: "os.environ/ANTHROPIC_API_KEY"
      max_tokens: 8192
    model_info:
      max_input_tokens: 200000
      max_output_tokens: 8192
      metadata:
        tier: "low"
        tags: ["translation", "summary", "quick", "classification"]

  # ============================================================
  # TIER MEDIUM — OpenAI fallbacks and review
  # ============================================================
  - model_name: "gpt-4o"
    litellm_params:
      model: "openai/gpt-4o"
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      metadata:
        tier: "medium"
        tags: ["multimodal", "fallback", "analysis"]

  - model_name: "gpt-4o-mini"
    litellm_params:
      model: "openai/gpt-4o-mini"
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      metadata:
        tier: "low"
        tags: ["classification", "formatting", "quick"]

  # GPT-5.2 Codex — Code review specialist (placeholder for 5.3 when API public)
  - model_name: "gpt-codex"
    litellm_params:
      model: "openai/codex-mini-latest"
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      metadata:
        tier: "medium"
        tags: ["code", "review-smart", "review-premium"]

  # ============================================================
  # TIER ULTRA-LOW — OpenRouter bulk/cheap models
  # ============================================================
{% if openrouter_api_key | default('') | length > 0 %}
  # Kimi K2.5 — Concierge primary (multimodal, agent swarm, tool calling)
  - model_name: "kimi-k2"
    litellm_params:
      model: "openrouter/moonshotai/kimi-k2"
      api_key: "os.environ/OPENROUTER_API_KEY"
      max_tokens: 4096
    model_info:
      max_output_tokens: 4096
      metadata:
        tier: "ultra-low"
        tags: ["concierge", "agent", "tool-calling", "multimodal"]

  # MiniMax M2.5 — Review standard, bulk, concierge fallback
  # 230B MoE (10B active), SWE-Bench 80.2%, Function Calling #1 (76.8%)
  - model_name: "minimax-m25"
    litellm_params:
      model: "openrouter/minimax/minimax-m1"
      api_key: "os.environ/OPENROUTER_API_KEY"
      max_tokens: 4096
    model_info:
      max_output_tokens: 4096
      metadata:
        tier: "ultra-low"
        tags: ["bulk", "review-standard", "data", "cheap", "fallback"]

  # DeepSeek R1 — Math, chain of reasoning, infra code
  - model_name: "deepseek-r1"
    litellm_params:
      model: "openrouter/deepseek/deepseek-r1"
      api_key: "os.environ/OPENROUTER_API_KEY"
      max_tokens: 4096
    model_info:
      max_output_tokens: 4096
      metadata:
        tier: "ultra-low"
        tags: ["math", "reasoning-chain", "infra"]

  # DeepSeek V3.2 — General code generation, pipeline gen
  - model_name: "deepseek-v3"
    litellm_params:
      model: "openrouter/deepseek/deepseek-chat"
      api_key: "os.environ/OPENROUTER_API_KEY"
      max_tokens: 4096
    model_info:
      max_output_tokens: 4096
      metadata:
        tier: "ultra-low"
        tags: ["code", "gen-infra", "pipeline"]

  # GLM-5 — Code gen, agent, record low hallucination (744B MoE, 40B active, MIT)
  - model_name: "glm-5"
    litellm_params:
      model: "openrouter/z-ai/glm-5"
      api_key: "os.environ/OPENROUTER_API_KEY"
      max_tokens: 4096
    model_info:
      max_output_tokens: 4096
      metadata:
        tier: "ultra-low"
        tags: ["code", "agent", "gen-infra", "review-standard"]

  # Grok 4.1 Fast — Web search + X search, #1 Search Arena (1163 Elo), 2M context
  - model_name: "grok-search"
    litellm_params:
      model: "openrouter/x-ai/grok-4.1-fast"
      api_key: "os.environ/OPENROUTER_API_KEY"
      max_tokens: 4096
    model_info:
      max_output_tokens: 4096
      metadata:
        tier: "ultra-low"
        tags: ["search", "web", "agent", "tool-calling"]

  # Perplexity Sonar Pro — Premium search with citations, agentic research
  - model_name: "perplexity-pro"
    litellm_params:
      model: "openrouter/perplexity/sonar-pro"
      api_key: "os.environ/OPENROUTER_API_KEY"
      max_tokens: 4096
    model_info:
      max_output_tokens: 4096
      metadata:
        tier: "medium"
        tags: ["search-premium", "research", "citations"]

  # Seedream 4.5 — Image generation via OpenRouter
  - model_name: "seedream"
    litellm_params:
      model: "openrouter/bytedance/seedream-4.5"
      api_key: "os.environ/OPENROUTER_API_KEY"
      max_tokens: 4096
    model_info:
      max_output_tokens: 4096
      metadata:
        tier: "ultra-low"
        tags: ["image-gen", "creative"]
{% endif %}

  # ============================================================
  # TIER FREE — Qwen3 Coder for trivial tasks
  # ============================================================
{% if openrouter_api_key | default('') | length > 0 %}
  - model_name: "qwen3-coder"
    litellm_params:
      model: "openrouter/qwen/qwen3-coder"
      api_key: "os.environ/OPENROUTER_API_KEY"
      max_tokens: 4096
    model_info:
      max_output_tokens: 4096
      metadata:
        tier: "free"
        tags: ["code", "code-gen", "trivial", "formatting", "bash"]
{% endif %}

  # ============================================================
  # GOOGLE — Video generation (Veo 3)
  # ============================================================
{% if google_gemini_api_key | default('') | length > 0 %}
  - model_name: "gemini-flash"
    litellm_params:
      model: "gemini/gemini-2.0-flash"
      api_key: "os.environ/GOOGLE_GEMINI_API_KEY"
    model_info:
      metadata:
        tier: "low"
        tags: ["quick", "multimodal", "fallback"]
{% endif %}

  # ============================================================
  # EMBEDDINGS — Used by semantic cache + RAG
  # ============================================================
  - model_name: "embedding"
    litellm_params:
      model: "openai/text-embedding-3-small"
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      metadata:
        tier: "embedding"
        tags: ["embedding"]

# ================================================================
# ROUTER SETTINGS — Intelligent routing
# ================================================================
router_settings:
  routing_strategy: "{{ litellm_routing_strategy }}"
  enable_tag_filtering: {{ litellm_enable_tag_filtering | lower }}
  # Cooldown a model for 60s after failure before retrying
  cooldown_time: 60
  # Number of retries across different deployments
  num_retries: {{ litellm_num_retries }}
  # Timeout per request
  timeout: {{ litellm_request_timeout }}
  # REX: LiteLLM health checks appellent TOUS les modèles toutes les ~38s.
  # Perplexity Sonar Pro seul a coûté $11.64 en 16h (1488 health checks à $0.01/check).
  # Désactivé globalement — on détecte les pannes via les fallbacks et métriques Prometheus.
  disable_cooldowns: false
  health_check_interval: 0

# ================================================================
# LITELLM SETTINGS — Cache, fallbacks, callbacks
# ================================================================
litellm_settings:
  drop_params: true
  set_verbose: false
  num_retries: {{ litellm_num_retries }}
  request_timeout: {{ litellm_request_timeout }}

  # Fallback chains: primary -> fallback1 -> fallback2
  # Strategy: open-source fallbacks first, Anthropic/OpenAI as last resort
  fallbacks:
    - claude-opus: ["claude-sonnet", "gpt-codex"]
{% if openrouter_api_key | default('') | length > 0 %}
    - claude-sonnet: ["gpt-4o", "glm-5"]
    - kimi-k2: ["glm-5", "minimax-m25"]
    - minimax-m25: ["glm-5", "deepseek-v3"]
    - deepseek-r1: ["glm-5", "qwen3-coder"]
    - deepseek-v3: ["glm-5", "qwen3-coder"]
    - glm-5: ["deepseek-v3", "minimax-m25"]
    - grok-search: ["perplexity-pro", "claude-haiku"]
    - perplexity-pro: ["grok-search", "claude-sonnet"]
    - qwen3-coder: ["deepseek-v3", "glm-5"]
{% else %}
    - claude-sonnet: ["gpt-4o"]
{% endif %}
    - gpt-4o: ["claude-sonnet"]
    - gpt-4o-mini: ["qwen3-coder"]
    - gpt-codex: ["claude-sonnet", "glm-5"]

  # Prometheus metrics
  success_callback: ["prometheus"]
  failure_callback: ["prometheus"]

  # Semantic cache: Redis exact match + Qdrant similarity search
  cache: true
  cache_params:
    type: "{{ litellm_cache_type }}"
    # Redis for exact match
    host: "os.environ/REDIS_HOST"
    port: "os.environ/REDIS_PORT"
    password: "os.environ/REDIS_PASSWORD"
    ttl: {{ litellm_cache_ttl }}
    # Qdrant for semantic similarity
    similarity_threshold: {{ litellm_cache_similarity_threshold }}
    qdrant_semantic_cache_embedding_model: "{{ qdrant_embedding_model }}"
    qdrant_collection_name: "{{ qdrant_cache_collection }}"
    qdrant_api_base: "http://qdrant:6333"
    qdrant_api_key: "os.environ/QDRANT_API_KEY"

  # Provider budget caps (USD per day)
  provider_budget_config:
    anthropic:
      budget_limit: {{ litellm_anthropic_budget_daily }}
      time_period: "1d"
    openai:
      budget_limit: {{ litellm_openai_budget_daily }}
      time_period: "1d"
{% if openrouter_api_key | default('') | length > 0 %}
    openrouter:
      budget_limit: {{ litellm_openrouter_budget_daily }}
      time_period: "1d"
{% endif %}
{% if google_gemini_api_key | default('') | length > 0 %}
    gemini:
      budget_limit: {{ litellm_google_budget_daily }}
      time_period: "1d"
{% endif %}
{% if byteplus_api_key | default('') | length > 0 %}
    # BytePlus — Seedance vidéo uniquement (ponctuel, budget mensuel séparé)
    # NON inclus dans le budget global journalier de $5
    bytedance:
      budget_limit: {{ litellm_byteplus_budget_monthly }}
      time_period: "30d"
{% endif %}

# ================================================================
# GENERAL SETTINGS — Auth, database, budget global
# ================================================================
general_settings:
  master_key: "os.environ/LITELLM_MASTER_KEY"
  database_url: "os.environ/DATABASE_URL"
  # Budget global cross-providers (hard cap absolu, toutes sources confondues)
  # Géré au niveau LiteLLM — bloque les appels si dépassé (retourne 429)
  # Les alertes 70%/90% sont gérées par n8n (cron 1h → PostgreSQL → Telegram)
  max_budget: {{ litellm_budget_global_daily }}
  budget_duration: "1d"
