# {{ ansible_managed }}
# docker-compose.yml — Phase B: Applications, Monitoring, System
# Infrastructure services (PG, Redis, Qdrant, Caddy) are managed by docker-compose-infra.yml
# Networks created by docker-compose-infra.yml (external: true)

services:
  # === APPLICATION LAYER ===
  n8n:
    image: {{ n8n_image }}
    container_name: {{ project_name }}_n8n
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    env_file:
      - /opt/{{ project_name }}/configs/n8n/n8n.env
    volumes:
      - /opt/{{ project_name }}/data/n8n:/home/node/.n8n
{% if openclaw_n8n_integration | default(false) and openclaw_volume_isolation | default(false) %}
      # Read-only access to OpenClaw workspace for RAG content indexing
      - {{ openclaw_workspace_dir }}:/home/node/openclaw-workspace:ro
{% endif %}
    networks:
      - backend
      - egress
    deploy:
      resources:
        limits:
          memory: {{ n8n_memory_limit }}
          cpus: "{{ n8n_cpu_limit }}"
        reservations:
          memory: {{ n8n_memory_reservation }}
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:5678/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

  litellm:
    image: {{ litellm_image }}
    container_name: {{ project_name }}_litellm
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    env_file:
      - /opt/{{ project_name }}/configs/litellm/litellm.env
    volumes:
      - /opt/{{ project_name }}/configs/litellm/litellm_config.yaml:/app/config.yaml:ro
    command: --config /app/config.yaml --port 4000
    networks:
      - backend
      - egress
      - sandbox
    deploy:
      resources:
        limits:
          memory: {{ litellm_memory_limit }}
          cpus: "{{ litellm_cpu_limit }}"
        reservations:
          memory: {{ litellm_memory_reservation }}
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import urllib.request; r=urllib.request.Request('http://127.0.0.1:4000/health',headers={'Authorization':'Bearer '+__import__('os').environ['LITELLM_MASTER_KEY']});urllib.request.urlopen(r)\" || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 90s

  openclaw:
    image: {{ openclaw_image }}
    container_name: {{ project_name }}_openclaw
    restart: unless-stopped
    init: true
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    env_file:
      - /opt/{{ project_name }}/configs/openclaw/openclaw.env
    volumes:
{% if openclaw_volume_isolation | default(false) %}
      # Volume isolation with Docker-in-Docker (DooD) path identity.
      # OpenClaw spawns sandbox containers via Docker socket. Sandbox bind-mount
      # paths use OPENCLAW_STATE_DIR as base. Docker daemon resolves these on the
      # HOST — so container paths MUST equal host paths for mounts to work.
      # The system dir is mounted at its own host path (identity mount).
      - {{ openclaw_system_dir }}:{{ openclaw_system_dir }}
      - {{ openclaw_workspace_dir }}:{{ openclaw_system_dir }}/workspace
      - {{ openclaw_state_dir }}/sessions:{{ openclaw_system_dir }}/sessions
      - {{ openclaw_state_dir }}/canvas:{{ openclaw_system_dir }}/canvas
      - {{ openclaw_state_dir }}/cron:{{ openclaw_system_dir }}/cron
      - {{ openclaw_state_dir }}/memory:{{ openclaw_system_dir }}/memory
{% else %}
      - /opt/{{ project_name }}/data/openclaw:/home/node/.openclaw
{% endif %}
      # Docker Outside of Docker (DooD): socket + CLI binary required for sandbox spawn.
      # OpenClaw calls child_process.spawn("docker", ["run", ...]) to create sandbox containers.
      # PATH inside image = /root/.bun/bin:... — /root/.bun/bin is mode 700 (root-owned),
      # inaccessible to node:1000. execvp("docker") hits EACCES before reaching /usr/local/bin.
      # Fix: mount host docker binary at /usr/local/bin/docker (comes after bun in PATH). REX-49.
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /usr/bin/docker:/usr/local/bin/docker:ro
    # REX: node:1000 must be in docker group to access /var/run/docker.sock
    # GID detected dynamically by docker-stack role (stat -c '%g' /var/run/docker.sock)
    group_add:
      - "{{ docker_socket_gid | default('989') }}"
    networks:
      - backend
      - egress
    deploy:
      resources:
        limits:
          memory: {{ openclaw_memory_limit }}
          cpus: "{{ openclaw_cpu_limit }}"
          pids: 512
        reservations:
          memory: {{ openclaw_memory_reservation }}
    # OpenClaw Gateway healthcheck: verify WebSocket upgrade (101) — not just HTTP 200 (SPA fallback)
    # HTTP GET always returns 200/index.html even when Gateway is in restart loop.
    # Only a successful WebSocket upgrade proves the Gateway is truly accepting connections.
    healthcheck:
      test: ["CMD-SHELL", "node -e \"const h=require('http');const r=h.request({host:'127.0.0.1',port:{{ openclaw_gateway_port }},path:'/',headers:{'Connection':'Upgrade','Upgrade':'websocket','Sec-WebSocket-Key':'dGhlIHNhbXBsZSBub25jZQ==','Sec-WebSocket-Version':'13'}});r.on('upgrade',(res)=>{process.exit(res.statusCode===101?0:1)});r.on('response',()=>process.exit(1));r.on('error',()=>process.exit(1));r.setTimeout(5000,()=>{r.destroy();process.exit(1)});r.end()\""]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  kaneo-api:
    image: {{ kaneo_api_image }}
    container_name: {{ project_name }}_kaneo_api
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    env_file:
      - /opt/{{ project_name }}/configs/kaneo/kaneo.env
    networks:
      - backend
    deploy:
      resources:
        limits:
          memory: {{ kaneo_memory_limit }}
          cpus: "{{ kaneo_cpu_limit }}"
        reservations:
          memory: {{ kaneo_memory_reservation }}
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:1337/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  kaneo-web:
    image: {{ kaneo_web_image }}
    container_name: {{ project_name }}_kaneo_web
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    environment:
      # REX: KANEO_API_URL must be the PUBLIC URL (https://...) — env.sh replaces the literal
      # "KANEO_API_URL" placeholder in the JS bundle at container start. If set to the internal
      # Docker hostname (http://kaneo-api:1337), the browser cannot resolve it → "Failed to fetch".
      KANEO_API_URL: "https://{{ kaneo_subdomain_override | default('hq') }}.{{ domain_name }}"
      KANEO_CLIENT_URL: "https://{{ kaneo_subdomain_override | default('hq') }}.{{ domain_name }}"
    networks:
      - frontend
      - backend
    deploy:
      resources:
        limits:
          memory: {{ kaneo_memory_limit }}
          cpus: "{{ kaneo_cpu_limit }}"
        reservations:
          memory: {{ kaneo_memory_reservation }}
    healthcheck:
      # REX: usekaneo/web image uses nginx on port 5173 (not 3000)
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:5173/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # === PERSONAL FINANCE ===
  sure-web:
    image: {{ sure_image }}
    container_name: {{ project_name }}_sure_web
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    env_file:
      - /opt/{{ project_name }}/configs/sure/sure.env
    volumes:
      - /opt/{{ project_name }}/data/sure:/rails/storage
    networks:
      - backend
      - frontend
    deploy:
      resources:
        limits:
          memory: {{ sure_web_memory_limit }}
          cpus: "{{ sure_web_cpu_limit }}"
        reservations:
          memory: {{ sure_web_memory_reservation }}
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://127.0.0.1:3000/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

  sure-worker:
    image: {{ sure_image }}
    container_name: {{ project_name }}_sure_worker
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    command: bundle exec sidekiq
    env_file:
      - /opt/{{ project_name }}/configs/sure/sure.env
    volumes:
      - /opt/{{ project_name }}/data/sure:/rails/storage
    networks:
      - backend
    deploy:
      resources:
        limits:
          memory: {{ sure_worker_memory_limit }}
          cpus: "{{ sure_worker_cpu_limit }}"
        reservations:
          memory: {{ sure_worker_memory_reservation }}
    healthcheck:
      test: ["CMD-SHELL", "kill -0 1 || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 60s

  # === MONITORING LAYER ===
  cadvisor:
    image: {{ cadvisor_image }}
    container_name: {{ project_name }}_cadvisor
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - DAC_OVERRIDE
      - FOWNER
    command:
      - "--docker_only=true"
      - "--housekeeping_interval=30s"
      - "--disable_metrics=advtcp,cpu_topology,cpuset,hugetlb,memory_numa,process,referenced_memory,resctrl,sched,tcp,udp"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /run/containerd/containerd.sock:/run/containerd/containerd.sock:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          memory: {{ cadvisor_memory_limit }}
          cpus: "{{ cadvisor_cpu_limit }}"
        reservations:
          memory: {{ cadvisor_memory_reservation }}
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:8080/healthz || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  victoriametrics:
    image: {{ victoriametrics_image }}
    container_name: {{ project_name }}_victoriametrics
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
      - DAC_OVERRIDE
      - FOWNER
    command:
      - "-storageDataPath=/storage"
      - "-retentionPeriod=30d"
      - "-httpListenAddr=:8428"
    volumes:
      - /opt/{{ project_name }}/data/victoriametrics:/storage
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          memory: {{ victoriametrics_memory_limit }}
          cpus: "{{ victoriametrics_cpu_limit }}"
        reservations:
          memory: {{ victoriametrics_memory_reservation }}
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://127.0.0.1:8428/-/healthy || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  loki:
    image: {{ loki_image }}
    container_name: {{ project_name }}_loki
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
      - DAC_OVERRIDE
      - FOWNER
    volumes:
      - /opt/{{ project_name }}/data/loki:/loki
      - /opt/{{ project_name }}/configs/loki/loki-config.yaml:/etc/loki/local-config.yaml:ro
    command: -config.file=/etc/loki/local-config.yaml -target=all
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          memory: {{ loki_memory_limit }}
          cpus: "{{ loki_cpu_limit }}"
        reservations:
          memory: {{ loki_memory_reservation }}
    # NOTE: grafana/loki 3.6.5 is distroless (no wget/curl/test/ls)
    # Use built-in 'loki -health' command added in v3.6.5 (backport PR #20590)
    healthcheck:
      test: ["CMD", "/usr/bin/loki", "-health"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 60s

  alloy:
    image: {{ alloy_image }}
    container_name: {{ project_name }}_alloy
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
      - SYS_PTRACE
      - DAC_READ_SEARCH
    volumes:
      - /opt/{{ project_name }}/configs/alloy/config.alloy:/etc/alloy/config.alloy:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /var/log:/var/log:ro
    command: run --server.http.listen-addr=0.0.0.0:12345 /etc/alloy/config.alloy
    networks:
      - backend
      - monitoring
    deploy:
      resources:
        limits:
          memory: {{ alloy_memory_limit }}
          cpus: "{{ alloy_cpu_limit }}"
        reservations:
          memory: {{ alloy_memory_reservation }}
    # NOTE: grafana/alloy may not have wget/curl
    # Process check as fallback; HTTP check done via smoke tests from host
    healthcheck:
      test: ["CMD-SHELL", "kill -0 1 || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s

  grafana:
    image: {{ grafana_image }}
    container_name: {{ project_name }}_grafana
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    environment:
      GF_SECURITY_ADMIN_USER: "{{ grafana_admin_user }}"
      GF_SECURITY_ADMIN_PASSWORD: "{{ grafana_admin_password }}"
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_SERVER_ROOT_URL: "https://{{ caddy_grafana_domain }}/"
    volumes:
      - /opt/{{ project_name }}/data/grafana:/var/lib/grafana
      - /opt/{{ project_name }}/configs/grafana/provisioning:/etc/grafana/provisioning:ro
      - /opt/{{ project_name }}/configs/grafana/dashboards:/var/lib/grafana/dashboards:ro
    networks:
      - frontend
      - monitoring
      - backend  # For PostgreSQL datasource (model_scores table)
    deploy:
      resources:
        limits:
          memory: {{ grafana_memory_limit }}
          cpus: "{{ grafana_cpu_limit }}"
        reservations:
          memory: {{ grafana_memory_reservation }}
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:3000/api/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s

  # === SYSTEM ===
  diun:
    image: {{ diun_image }}
    container_name: {{ project_name }}_diun
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - DAC_OVERRIDE
      - FOWNER
    environment:
      TZ: "{{ timezone }}"
      CONFIG: "/diun.yml"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /opt/{{ project_name }}/data/diun:/data
      - /opt/{{ project_name }}/configs/diun/diun.yml:/diun.yml:ro
    deploy:
      resources:
        limits:
          memory: {{ diun_memory_limit }}
          cpus: "{{ diun_cpu_limit }}"
        reservations:
          memory: {{ diun_memory_reservation }}
    healthcheck:
      test: ["CMD-SHELL", "kill -0 1 || exit 1"]
      interval: 60s
      timeout: 5s
      retries: 3
      start_period: 30s

networks:
  frontend:
    name: {{ project_name }}_frontend
    external: true
  backend:
    name: {{ project_name }}_backend
    external: true
  monitoring:
    name: {{ project_name }}_monitoring
    external: true
  egress:
    name: {{ project_name }}_egress
    external: true
  sandbox:
    name: {{ project_name }}_sandbox
    external: true
